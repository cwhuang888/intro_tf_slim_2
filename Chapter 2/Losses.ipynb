{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function Module in TF-Slim\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../images/tensorflow.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Losses Functions**\n",
    "\n",
    "TF-Slim provides an easy-to-use mechanism for defining and keeping track of loss functions via the losses module.\n",
    "\n",
    "All of the loss functions take a pair of predictions and ground truth labels, from which the loss is computed.\n",
    "\n",
    "In machine learning, a loss function is used to measure the \"cost\" or degree of fit of the model. Therefore it is important to use the appropriate cost function based on the predictive task at hand.\n",
    "\n",
    "The chose of loss function should optimize the metric we care about (such as accuracy, residue error). \n",
    "\n",
    "Every deep learning model is an optimization problem that seeks to minimize a loss function. The loss score is what is used to backpropagate the error signal throughout the neural network and update the model weights. It is therefore a crucial component of the learning process. \n",
    "\n",
    "\n",
    "<img src=\"../images/loss_acc.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append(\"../\") \n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Import The Flower CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.slim_models import CNNClassifier\n",
    "\n",
    "image_shape = (64,64,3)\n",
    "num_class = 5\n",
    "\n",
    "CNN_model = CNNClassifier(\"flowers\", image_shape , num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common classfier choice for multi-class task is the **Softmax classifier**.\n",
    "The softmax classifier is a generalization of a binary classifier to multiple classes.\n",
    "The softmax classifier has an interpretable output of normalized class probabilities.\n",
    "\n",
    "**Softmax function**\n",
    "$$f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$$\n",
    "\n",
    "**Softmax function** takes a vector of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum to one. Therefore, it guarantees that the sum of all class probabilities is 1.That's why it's used for multi-class classification because you expect your samples to belong to a single class at the time.\n",
    "\n",
    "**Cross-Entropy Loss**\n",
    "$$L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right)$$\n",
    "\n",
    "The Softmax classifier minimizes the **cross-entropy** between the estimated class probabilities and the “true” distribution, which is the one-hot encoding of the target labels.\n",
    "\n",
    "This loss is equivalent to minimizing the KL divergence (distance) between the two distributions.\n",
    "\n",
    "$$H(p,q) = - \\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "Another interpretation is the cross-entropy objective wants the predicted distribution to have all of its mass on the correct label.\n",
    "\n",
    "**Probabilistic interpretation**\n",
    "Softmax classifier gives the probability assigned to the correct label $y_i$ given the image $x_i$ and parameterized by $W$.\n",
    "\n",
    "$$P(y_i \\mid x_i; W) = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j} }$$\n",
    "\n",
    "We are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing **Maximum Likelihood Estimation (MLE)**. With added L2 regularization (which equates to a Gaussian prior over the weight matrix $W$), we are instead performing the **Maximum a posteriori (MAP)** estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input batch images\n",
    "inputs = tf.placeholder(tf.float32, shape=(None,) + image_shape)\n",
    "\n",
    "# target batch labels\n",
    "target = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "# Make the model.\n",
    "logits, _ = CNN_model.graph(inputs, weight_decay=0.05, dropout=0.5)\n",
    "\n",
    "# transform labels into one-hot-encoding\n",
    "one_hot_labels = slim.one_hot_encoding(targets, num_class)\n",
    "\n",
    "# Add the loss function to the graph.\n",
    "loss = slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "\n",
    "# The total loss is the model's loss plus any regularization losses.\n",
    "total_loss = slim.losses.get_total_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "**Sigmoid function** or logistic function only ouputs a single value (between 0 and 1) , independent of all other values. It is often used as an activation function with saturation points at both extremes. It is also used for binary classification.\n",
    "\n",
    "<img src=\"../images/sigmoid.jpg\" width=\"200\">\n",
    "\n",
    "$$f_j(z)=\\frac {1}{1+e^{-z_j}}$$\n",
    "\n",
    "**Sigmoid function** can be thought of has a special case of softmax where the number of class equals to 2.  Sigmoid functions takes in a single output neuron and the prediction is defined by an arbitrary threshold (often 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inpute images\n",
    "inputs = tf.placeholder(tf.float32, shape=(None,) + image_shape)\n",
    "\n",
    "# target binary labels\n",
    "binary_labels = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "# Make the model (e.g. not implemented BinaryClassificationModel)\n",
    "logits, nodes = BinaryClassificationModel(inputs)\n",
    "\n",
    "\n",
    "# Add the loss function to the graph.\n",
    "loss = slim.losses.sigmoid_cross_entropy(logits, binary_labels)\n",
    "\n",
    "# The total loss is the model's loss plus any regularization losses.\n",
    "total_loss = slim.losses.get_total_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Label Classification\n",
    "Multi-Label Classification is when a sample observation can belong to multiple classes at the same time.\n",
    "\n",
    "We can rephrase multi-label learning as the problem of finding a model that maps inputs x to binary vectors y, rather than scalar outputs as in the ordinary classification problem.\n",
    "With this interpretation, the solution is to apply an independent sigmoid function for each label.\n",
    "\n",
    "Sigmoid functions give predictions independent of all other classes. We lose the probabilistic interpretation because the prediction sum can range anywhere from 0 to K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multi-label classification\n",
    "loss = slim.losses.sigmoid_cross_entropy(logits, multi_class_labels) #[batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Predictions\n",
    "All the losses we've seen so far combine multiple steps into one to compute the loss score.\n",
    "What if we are interested in the intermediate steps such as the predictions probabilities and the actual label predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions probabilities\n",
    "predictions_probabilities = slim.softmax(logits)\n",
    "\n",
    "# predictions as intermediate steps\n",
    "predictions = tf.argmax(predictions_probabilities, axis=1)\n",
    "\n",
    "# log-loss (multi-class cross-entropy)\n",
    "loss = slim.losses.log_loss(predictions_probabilities, one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Loss\n",
    "**Hinge Loss** is used in Multiclass Support Vector Machine (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\\Delta$.\n",
    "\n",
    "<img src=\"../images/hinge.png\" width=\"400\">\n",
    "\n",
    "For the score function $s_j = f(x_i, W)_j$. The Multiclass SVM loss for the i-th example is then formalized as follows:\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiclass SVM with hinge loss\n",
    "sum_of_squares_loss = slim.losses.hinge_loss(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss\n",
    "Not all predictive tasks involve classification with distinct labels. Sometimes we need to predict a continuous variable. For example the price of a house given a set of features, or predict how much snow will fall given weather data.\n",
    "\n",
    "**Mean squared error (MSE)** measures the average of the squares of the errors or deviations. In the context of regression analysis, it measures the quality of an estimator—it is always non-negative, and values closer to zero are better.\n",
    "\n",
    "$$\\operatorname {MSE}={\\frac  {1}{n}}\\sum _{{i=1}}^{n}({\\hat  {Y_{i}}}-Y_{i})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean squared error for regression tasks\n",
    "slim.losses.mean_squared_error(predictions, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Losses for Multi-Task Model\n",
    "More complex problems sometime require to minimize multiple objectives (cam be both categorical and continous). This are called **multi-task models** and they produces multiple outputs. \n",
    "\n",
    "TF-Slim allows to easily combine multiple loss functions together and optimize over both of them. \n",
    "\n",
    "When you create a loss function via TF-Slim, TF-Slim adds the loss to a special TensorFlow collection of loss functions. This enables you to either manage the total loss manually, or allow TF-Slim to manage them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define two loss functions\n",
    "classification_loss = slim.losses.softmax_cross_entropy(categorical_predictions, categorical_labels)\n",
    "regression_loss = slim.losses.mean_squared_error(continous_predictions, continous_labels)\n",
    "\n",
    "# Compute the total loss of the model\n",
    "total_loss = classification_loss + regression_loss\n",
    "\n",
    "# or equivalent with slim built in function\n",
    "total_loss = slim.losses.get_total_loss(add_regularization_losses=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costum Loss Functions with Regularization Loss\n",
    "TF-Slim allows you also to construct your custom loss function.\n",
    "\n",
    "For example if you want to tailor your objective to a specific task, where common loss functions are not appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define two loss functions with a custom one\n",
    "classification_loss = slim.losses.softmax_cross_entropy(predictions_1, labels_1)\n",
    "custom_loss = MyCustomLossFunction(predictions_2, labels_2)\n",
    "\n",
    "# Letting TF-Slim know about the additional loss.\n",
    "slim.losses.add_loss(custom_loss) \n",
    "\n",
    "# Compute regularization loss\n",
    "regularization_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "\n",
    "# get total model loss\n",
    "total_loss = classification_loss + custom_loss + regularization_loss\n",
    "\n",
    "# OR use Slim built in function to compute total loss with regularization.\n",
    "total_loss = slim.losses.get_total_loss(add_regularization_losses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other Functions Provided by Slim.losses Module\n",
    "\n",
    "TF-Slim provides other useful loss function and distance metrics that I will let you explore on your own. Below are a few examples:\n",
    "\n",
    "```\n",
    "slim.losses.sparse_softmax_cross_entropy()\n",
    "slim.losses.mean_pairwise_squared_error()\n",
    "slim.losses.cosine_distance()\n",
    "slim.losses.absolute_difference()\n",
    "slim.losses.compute_weighted_loss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next Lesson\n",
    "### Build Compact Training Routings in TF-Slim\n",
    "-  Construct training routines and train your first deep neural network.\n",
    "\n",
    "<img src=\"../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TFr12-py2-env]",
   "language": "python",
   "name": "conda-env-TFr12-py2-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
