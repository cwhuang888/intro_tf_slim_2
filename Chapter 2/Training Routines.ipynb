{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routines in TF-Slim\n",
    "\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../images/tensorflow.png\" width=\"400\">\n",
    "\n",
    "**Training Deep Learning Models**\n",
    "\n",
    "Training Tensorflow model requirements\n",
    "- a model represented has a computational graph.\n",
    "- a loss function to minimize and optimize over.\n",
    "- the gradient computation of the model weights relative to the loss to perform backpropagation of the error signal.\n",
    "- a training routine that iteratively does all of the above and updates the weights accordingly.\n",
    "\n",
    "In the previous lession we looked at the loss functions provided by TF-Slim.\n",
    "In this lesson, we'll see that TF-Slim also provides training routines that simplifies the training process for neural networks.\n",
    "\n",
    "\n",
    "<img src=\"../images/backprop.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append(\"../\") \n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**\n",
    "\n",
    "TF-Slim provides a simple but powerful set of tools for training models.\n",
    "\n",
    "TF-Slim training loop allows the user to pass in the `train_op` and runs the optimization according to user-specified arguments, such as the loss function and the optimization method.\n",
    "\n",
    "The training operation includes:\n",
    "\n",
    "1. Iteratively measures the loss\n",
    "2. Computes gradients\n",
    "3. Update the model weights\n",
    "4. Saves the model to disk\n",
    "\n",
    "\n",
    "Note that the training loop uses the tf.Supervisor\n",
    "and its managed_session in its implementation to ensure the ability of worker\n",
    "processes to recover from failures.\n",
    "\n",
    "## Training Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "images, labels = LoadData(...)\n",
    "\n",
    "# Create a model and make predictions\n",
    "predictions = MyModel(images)\n",
    "\n",
    "# Define a losses function\n",
    "slim.losses.log_loss(predictions, labels)\n",
    "\n",
    "# Get total model loss and regularization loss\n",
    "total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "# Define Optimization method (SGD, Momentum, RMSProp, AdaGrad, Adam optimizer)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# create_train_op at each steps:\n",
    "# compute loss, comput gradients and compute update_ops\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "# Where checkpoints and event files are stored.\n",
    "logdir = \"/logdir/path\" \n",
    "\n",
    "slim.learning.train(\n",
    "    train_op,\n",
    "    logdir,\n",
    "    number_of_steps=1000, # number of gradient steps\n",
    "    save_summaries_secs=60, # compute summaries every 60 secs\n",
    "    save_interval_secs=300) # save model checkpoint every 5 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Gradients with the Training Operation\n",
    "\n",
    "TF-Slim training function also provides the ability to manipulate the gradients.\n",
    "\n",
    "**Gradient norm clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the train_op and clip the gradient norms\n",
    "# L2-norm greater than 4 will be clipped to avoid 'exploding gradients` especially in RNNs\n",
    "train_op = slim.learning.create_train_op(\n",
    "  total_loss,\n",
    "  optimizer,\n",
    "  clip_gradient_norm=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the train_op and scale the gradients\n",
    "# scaling the gradients redistributes the weight importance\n",
    "\n",
    "# mapping from variable name to scaling coefficient\n",
    "gradient_multipliers = {\n",
    "    'conv1/weights': 2.4,\n",
    "    'fc8/weights': 5.1,\n",
    "}\n",
    "\n",
    "train_op = slim.learning.create_train_op(\n",
    "  total_loss,\n",
    "  optimizer,\n",
    "  gradient_multipliers=gradient_multipliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modifying the Update Operation\n",
    "\n",
    "TF-Slim also provide the option of modifying the update operation. This is the operation that performs the learning step at every iteration.\n",
    "\n",
    "You can:\n",
    "\n",
    "- Override the default update ops with a custom specialized update.\n",
    "- Remove the update operation completely. For example in the batch normalizing layer, it is required to perform a series of non-gradient updates during training, such as computing the moving mean and moving variance.\n",
    "\n",
    "Since BachNorm is already an implemented layer in TF-Slim, the non-gradient update is done automatically by TensorFlow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use an alternative set of update ops:\n",
    "train_op = slim.learning.create_train_op(\n",
    "    total_loss,\n",
    "    optimizer,\n",
    "    update_ops=my_other_update_ops)\n",
    "\n",
    "# Force TF-Slim NOT to use ANY update_ops:\n",
    "train_op = slim.learning.create_train_op(\n",
    "    total_loss,\n",
    "    optimizer,\n",
    "    update_ops=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load CNN Flower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers\n",
      "name = CNN_flowers_classifier/conv1/conv1_1/Relu:0             shape = (?, 64, 64, 64)\n",
      "name = CNN_flowers_classifier/pool1/MaxPool:0                  shape = (?, 32, 32, 64)\n",
      "name = CNN_flowers_classifier/conv2/conv2_1/Relu:0             shape = (?, 32, 32, 128)\n",
      "name = CNN_flowers_classifier/conv2/conv2_2/Relu:0             shape = (?, 32, 32, 128)\n",
      "name = CNN_flowers_classifier/pool2/MaxPool:0                  shape = (?, 16, 16, 128)\n",
      "name = CNN_flowers_classifier/conv3/conv3_1/Relu:0             shape = (?, 16, 16, 256)\n",
      "name = CNN_flowers_classifier/conv3/conv3_2/Relu:0             shape = (?, 16, 16, 256)\n",
      "name = CNN_flowers_classifier/conv3/conv3_3/Relu:0             shape = (?, 16, 16, 256)\n",
      "name = CNN_flowers_classifier/pool3/MaxPool:0                  shape = (?, 8, 8, 256)\n",
      "name = CNN_flowers_classifier/fc4/Relu:0                       shape = (?, 1, 1, 1024)\n",
      "name = CNN_flowers_classifier/fc5/Relu:0                       shape = (?, 1, 1, 1024)\n",
      "name = CNN_flowers_classifier/prediction/squeezed:0            shape = (?, 5)\n",
      "\n",
      "\n",
      "Parameters\n",
      "name = CNN_flowers_classifier/conv1/conv1_1/weights:0          shape = (3, 3, 3, 64)\n",
      "name = CNN_flowers_classifier/conv1/conv1_1/biases:0           shape = (64,)\n",
      "name = CNN_flowers_classifier/conv2/conv2_1/weights:0          shape = (3, 3, 64, 128)\n",
      "name = CNN_flowers_classifier/conv2/conv2_1/biases:0           shape = (128,)\n",
      "name = CNN_flowers_classifier/conv2/conv2_2/weights:0          shape = (3, 3, 128, 128)\n",
      "name = CNN_flowers_classifier/conv2/conv2_2/biases:0           shape = (128,)\n",
      "name = CNN_flowers_classifier/conv3/conv3_1/weights:0          shape = (3, 3, 128, 256)\n",
      "name = CNN_flowers_classifier/conv3/conv3_1/biases:0           shape = (256,)\n",
      "name = CNN_flowers_classifier/conv3/conv3_2/weights:0          shape = (3, 3, 256, 256)\n",
      "name = CNN_flowers_classifier/conv3/conv3_2/biases:0           shape = (256,)\n",
      "name = CNN_flowers_classifier/conv3/conv3_3/weights:0          shape = (3, 3, 256, 256)\n",
      "name = CNN_flowers_classifier/conv3/conv3_3/biases:0           shape = (256,)\n",
      "name = CNN_flowers_classifier/fc4/weights:0                    shape = (8, 8, 256, 1024)\n",
      "name = CNN_flowers_classifier/fc4/biases:0                     shape = (1024,)\n",
      "name = CNN_flowers_classifier/fc5/weights:0                    shape = (1, 1, 1024, 1024)\n",
      "name = CNN_flowers_classifier/fc5/biases:0                     shape = (1024,)\n",
      "name = CNN_flowers_classifier/prediction/weights:0             shape = (1, 1, 1024, 5)\n",
      "name = CNN_flowers_classifier/prediction/biases:0              shape = (5,)\n"
     ]
    }
   ],
   "source": [
    "from utils.slim_models import CNNClassifier\n",
    "\n",
    "image_shape = (64,64,3)\n",
    "num_class = 5\n",
    "\n",
    "CNN_model = CNNClassifier(\"flowers\", image_shape , num_class)\n",
    "CNN_model.examine_model_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the model.\n",
    "logits, _ = CNN_model.graph(inputs, weight_decay, dropout)\n",
    "\n",
    "# Add the loss function to the graph.\n",
    "one_hot_labels = slim.one_hot_encoding(targets, output_dim)\n",
    "loss = slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "\n",
    "# The total loss is the model's loss plus any regularization losses.\n",
    "total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "# Specify the optimizer and create the train op:\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "# Run the training inside a session.\n",
    "final_loss = slim.learning.train(\n",
    "    train_op,\n",
    "    logdir=checkpoint_dir,\n",
    "    number_of_steps=iterations,\n",
    "    save_summaries_secs=5,\n",
    "    log_every_n_steps=log_frq)\n",
    "\n",
    "print(\"Finished training. Last batch loss:\", final_loss)\n",
    "print(\"Checkpoint saved in %s\" % checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.slim_training_evaluation import ModelTrainerEvaluater\n",
    "from utils.slim_data_provider import DatasetProvider\n",
    "\n",
    "checkpoint_dir=\"../models/flowers/\"\n",
    "data_dir = \"../data/flowers/\"\n",
    "\n",
    "CNN_trainer = ModelTrainerEvaluater(model=CNN_model,\n",
    "                           dataset_provider = DatasetProvider(data_dir),\n",
    "                           data_name=\"flowers\",\n",
    "                           checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN\n",
    "\n",
    "This helper function combines all the parameters into one method. You can start training by running the command below\n",
    "\n",
    "**Training tips**\n",
    "- Train your model by monitoring the loss values. The model starts learning when the loss starts going down.\n",
    "- Experiment with different parameter configurations, the one given are just to get you started. \n",
    "- This is a large neural network, therefore the training can take several hours. It's recommended to train the CNN on a GPU either locally (if you have one), or in the cloud (ie AWS, Google Cloud Platform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/marvinbertin/anaconda3/envs/TFr12-py2-env/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py:344 in __init__.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n",
      "('Finished training. Last batch loss:', 5.0561266)\n",
      "Checkpoint saved in ../models/flowers/\n"
     ]
    }
   ],
   "source": [
    "CNN_trainer.train(weight_decay=0.005,\n",
    "                  dropout=0.5,\n",
    "                  learning_rate=0.0005,\n",
    "                  iterations=100,\n",
    "                  log_frq=100,\n",
    "                  batch_size=32,\n",
    "                  data_type='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "### Evaluation Metrics in TF-Slim\n",
    "-  Explore different evaluation metrics provided by TF-Slim\n",
    "\n",
    "<img src=\"../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TFr12-py2-env]",
   "language": "python",
   "name": "conda-env-TFr12-py2-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
