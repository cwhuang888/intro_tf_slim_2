{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Routines in TF-Slim\n",
    "\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../images/tensorflow.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating Deep Learning Models**\n",
    "\n",
    "Evaluating Deep Learning models allows us to measure the model performance and let us know if the neural network is learning useful information for solving the task.\n",
    "\n",
    "Deep neural networks can be very large and take a long time to train. Therefore it is recommended to evaluate the model's performance regularly while training.\n",
    "\n",
    "It is important to monitor the 'health' of the training because optimization could stop functioning properly.\n",
    "\n",
    "For example for the following reasons:\n",
    "\n",
    "- overfitting (use early stopping, regularization, more data)\n",
    "- vanishing or exploding gradients (clip gradient norm, change activation function, residual skip connection)\n",
    "- non-converging learning (bad initialization, large learning rate, tune optimizer, bug in network)\n",
    "- reaching local minima (update learning rate, dropout)\n",
    "- covariate shift in very deep network (batch normalization)\n",
    "- low performance, high bias (modify model architecture, larger network)\n",
    "\n",
    "TF-Slim has an evaluation module that contains helper functions for evaluating TensorFlow\n",
    "models using a variety of metrics and summarizing the results.\n",
    "\n",
    "**Evaluation Loop**\n",
    "\n",
    "TF-Slim provides an evaluation module, which contains helper functions for writing model evaluation scripts using metrics from the metrics module.\n",
    "\n",
    "Evaluation loop\n",
    "- runs evaluation periodically\n",
    "- evaluates metrics over batches of data\n",
    "- summarizes metric results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.append(\"../\") \n",
    "\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for a Single Run\n",
    "\n",
    "In the simplest use case, we use a model to create the predictions, then specify\n",
    "the metrics and finally call the `evaluation` method:\n",
    "\n",
    "`slim.evaluation.evaluation()` will perform a single evaluation run.\n",
    "\n",
    "**A single evaluation consists of several steps**\n",
    "\n",
    "1. an initialization op that initialize local and global variables.\n",
    "2. an evaluation op which is executed `num_evals` times.\n",
    "3. a finalization op which is executed at end of the evaluation loop.\n",
    "4. the execution of a summary op which is written out using a summary writer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and obtain the predictions:\n",
    "images, labels = LoadData(...)\n",
    "predictions = MyModel(images)\n",
    "\n",
    "# Choose the metrics to compute:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    \"accuracy\": slim.metrics.accuracy(predictions, labels),\n",
    "    \"mse\": slim.metrics.mean_squared_error(predictions, labels),\n",
    "})\n",
    "\n",
    "# Initialize variables\n",
    "inital_op = tf.group(\n",
    "    tf.global_variables_initializer(),\n",
    "    tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run evaluation\n",
    "    metric_values = slim.evaluation.evaluation(\n",
    "        sess,\n",
    "        num_evals=10,\n",
    "        inital_op=initial_op,\n",
    "        eval_op=names_to_updates.values(),\n",
    "        final_op=name_to_values.values())\n",
    "    \n",
    "    # print final metric values\n",
    "    for metric, value in zip(names_to_values.keys(), metric_values):\n",
    "        logging.info('Metric %s has value: %f', metric, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Checkpointed Model with Metrics\n",
    "\n",
    "Often, one wants to evaluate a model checkpoint saved on disk.\n",
    "\n",
    "The evaluation can be performed periodically during training on a set schedule.\n",
    "\n",
    "Instead of calling the `evaluation()` method, we now call `evaluation_loop()` method. We now provide in addition the logging and checkpoint directory, as well as, a evaluation time interval.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model and obtain the predictions:\n",
    "images, labels = LoadData(...)\n",
    "predictions = MyModel(images)\n",
    "\n",
    "# Choose the metrics to compute:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "      \"accuracy\": slim.metrics.accuracy(predictions, labels),\n",
    "      \"mse\": slim.metrics.mean_squared_error(predictions, labels),\n",
    "  })\n",
    "\n",
    "# model checkpoints\n",
    "checkpoint_dir = '/tmp/my_model_dir/'\n",
    "\n",
    "# logging\n",
    "log_dir = '/tmp/my_model_eval/'\n",
    "\n",
    "# evaluate for 1000 batches:\n",
    "num_evals = 1000\n",
    "\n",
    "# Evaluate every 10 minutes:\n",
    "slim.evaluation.evaluation_loop(\n",
    "      master='',\n",
    "      checkpoint_dir,\n",
    "      logdir,\n",
    "      num_evals=num_evals, # number of batches to evaluate\n",
    "      eval_op=names_to_updates.values(),\n",
    "      eval_interval_secs=600) # How often to run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Checkpointed Model with Summaries\n",
    "\n",
    "In addition to computing the metrics, the evaluation loop can also construct metrics, scalar, and histogram summaries of the model and save them to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "images, labels = load_data(...)\n",
    "\n",
    "# Define the network\n",
    "predictions = MyModel(images)\n",
    "\n",
    "# Choose the metrics to compute:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    'accuracy': slim.metrics.accuracy(predictions, labels),\n",
    "    'precision': slim.metrics.precision(predictions, labels),\n",
    "    'recall': slim.metrics.recall(predictions, targets),\n",
    "})\n",
    "\n",
    "\n",
    "# Define the summaries to write:\n",
    "for metric_name, metric_value in names_to_values.iteritems():\n",
    "    tf.summary.scalar(metric_name, metric_value)\n",
    "    \n",
    "# Define other summaries to write (loss, activations, gradients)\n",
    "tf.summary.scalar(...)\n",
    "tf.summary.histogram(...)\n",
    "\n",
    "checkpoint_dir = '/tmp/my_model_dir/'\n",
    "log_dir = '/tmp/my_model_eval/'\n",
    "\n",
    "# evaluate for 1000 batches:\n",
    "num_evals = 1000\n",
    "\n",
    "# Setup the global step.\n",
    "slim.get_or_create_global_step()\n",
    "\n",
    "slim.evaluation.evaluation_loop(\n",
    "    master='',\n",
    "    checkpoint_dir,\n",
    "    log_dir,\n",
    "    num_evals=num_evals,\n",
    "    eval_op=names_to_updates.values(),\n",
    "    summary_op=tf.summary.merge(summary_ops), # Merge summaries (list of summary operations)\n",
    "    eval_interval_secs=600) # How often to run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating at a Given Checkpoint.\n",
    "\n",
    "When a model has already been trained, and we only wish to evaluate it from its last checkpoint, TF-Slim has provided us with a method calle `evaluate_once()`. It only evaluates the model at the given checkpoint path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.slim_models import CNNClassifier\n",
    "\n",
    "image_shape = (64,64,3)\n",
    "num_class = 5\n",
    "\n",
    "CNN_model = CNNClassifier(\"flowers\", image_shape , num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits, nodes = CNN_model(inputs, dropout = 0.5, is_training=False)\n",
    "predictions = tf.argmax(logits, 1)\n",
    "\n",
    "# Define streaming metrics\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, targets),\n",
    "    'eval/Recall@3': slim.metrics.streaming_sparse_recall_at_k(\n",
    "            tf.to_float(logits), tf.expand_dims(targets,1), 3),\n",
    "    'eval/Precision': slim.metrics.streaming_precision(predictions, targets),\n",
    "    'eval/Recall': slim.metrics.streaming_recall(predictions, targets)\n",
    "})\n",
    "\n",
    "\n",
    "print('Running evaluation Loop...')\n",
    "# Only load latest checkpoint\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "metric_values = slim.evaluation.evaluate_once(\n",
    "    num_evals=num_evals,\n",
    "    master='',\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    logdir=checkpoint_dir,\n",
    "    eval_op=names_to_updates.values(),\n",
    "    final_op=names_to_values.values())\n",
    "\n",
    "# print final metric values\n",
    "names_to_values = dict(zip(names_to_values.keys(), metric_values))\n",
    "for name in names_to_values:\n",
    "    print('%s: %f' % (name, names_to_values[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate CNN Flower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.slim_training_evaluation import ModelTrainerEvaluater\n",
    "from utils.slim_data_provider import DatasetProvider\n",
    "\n",
    "checkpoint_dir=\"../models/flowers/\"\n",
    "data_dir = \"../data/flowers/\"\n",
    "\n",
    "CNN_trainer = ModelTrainerEvaluater(model = CNN_model,\n",
    "                           dataset_provider = DatasetProvider(data_dir),\n",
    "                           data_name=\"flowers\",\n",
    "                           checkpoint_dir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation Loop...\n",
      "WARNING:tensorflow:From /Users/marvinbertin/anaconda3/envs/TFr12-py2-env/lib/python2.7/site-packages/tensorflow/contrib/slim/python/slim/evaluation.py:335 in evaluate_once.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "INFO:tensorflow:Starting evaluation at 2017-02-18-19:52:17\n",
      "INFO:tensorflow:Executing eval ops\n",
      "INFO:tensorflow:Executing eval_op 1/20\n",
      "INFO:tensorflow:Executing eval_op 2/20\n",
      "INFO:tensorflow:Executing eval_op 3/20\n",
      "INFO:tensorflow:Executing eval_op 4/20\n",
      "INFO:tensorflow:Executing eval_op 5/20\n",
      "INFO:tensorflow:Executing eval_op 6/20\n",
      "INFO:tensorflow:Executing eval_op 7/20\n",
      "INFO:tensorflow:Executing eval_op 8/20\n",
      "INFO:tensorflow:Executing eval_op 9/20\n",
      "INFO:tensorflow:Executing eval_op 10/20\n",
      "INFO:tensorflow:Executing eval_op 11/20\n",
      "INFO:tensorflow:Executing eval_op 12/20\n",
      "INFO:tensorflow:Executing eval_op 13/20\n",
      "INFO:tensorflow:Executing eval_op 14/20\n",
      "INFO:tensorflow:Executing eval_op 15/20\n",
      "INFO:tensorflow:Executing eval_op 16/20\n",
      "INFO:tensorflow:Executing eval_op 17/20\n",
      "INFO:tensorflow:Executing eval_op 18/20\n",
      "INFO:tensorflow:Executing eval_op 19/20\n",
      "INFO:tensorflow:Executing eval_op 20/20\n",
      "INFO:tensorflow:Executing final op\n",
      "INFO:tensorflow:Executing summary op\n",
      "INFO:tensorflow:Finished evaluation at 2017-02-18-19:52:42\n",
      "eval/Precision: 0.615970\n",
      "eval/Accuracy: 0.253125\n",
      "eval/Recall@3: 0.643750\n",
      "eval/Recall: 1.000000\n"
     ]
    }
   ],
   "source": [
    "CNN_trainer.evaluate(num_evals=20,\n",
    "                     data_type='validation',\n",
    "                     dropout = 0.5,\n",
    "                     batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "### Fine-Tuning and Transfer Learning in TF-Slim\n",
    "-  Explore how to fine-tune pre-trained models and use transfer learning to train on a new task\n",
    "\n",
    "<img src=\"../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TFr12-py2-env]",
   "language": "python",
   "name": "conda-env-TFr12-py2-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
